{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tumor_Cell_State_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shantanu9326/Breast-Cancer-Wisconsin-Diagnostic-Data-Set/blob/master/Tumor_Cell_State_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwkLJrdjjFEs",
        "colab_type": "code",
        "outputId": "59f38edd-0346-4ded-9167-338e8a40318e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Running or Importing .py Files with Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZQvTIw-pO40",
        "colab_type": "text"
      },
      "source": [
        "##Part A\n",
        "\n",
        "###1.1 Data Munging\n",
        "\n",
        "• Read the training and testing data. Print the number of features in the dataset.\n",
        "\n",
        "• For the data label, print the total number of 1's and 0's in the training and testing data. Comment on the class\n",
        "distribution. Is it balanced or unbalanced?\n",
        "\n",
        "• Print the number of features with missing entries.\n",
        "\n",
        "• Fill the missing entries. For filling any feature, you can use either mean or median value of the feature values\n",
        "from observed entries.\n",
        "\n",
        "• Normalize the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmwCrlsAoPr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Importing relevant modules ###\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "\n",
        "## Visualisation\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#Ignore Warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESJDqMd2opyr",
        "colab_type": "code",
        "outputId": "8df3d52b-877b-4944-a592-6c1cf5720696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3247
        }
      },
      "source": [
        "## Read the training and testing data. Print the number of features in the dataset\n",
        "train = pd.read_csv(\"/content/drive/My Drive/app/train_wbcd.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/app/test_wbcd.csv\")\n",
        "\n",
        "dataset = pd.concat([train,test])\n",
        "print(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Patient_ID Diagnosis      f1     f2      f3      f4       f5       f6  \\\n",
            "0       909410         B  14.020  15.66   89.59   606.5  0.07966  0.05581   \n",
            "1     84358402         M  20.290  14.34  135.10  1297.0  0.10030  0.13280   \n",
            "2      8912284         B  12.890  15.70   84.08   516.6  0.07818  0.09580   \n",
            "3     90317302         B  10.260  12.22   65.75   321.6  0.09996  0.07542   \n",
            "4       914102         B  13.160  20.54   84.06   538.7  0.07335  0.05275   \n",
            "5       924342         B   9.333  21.94   59.01   264.0  0.09240  0.05605   \n",
            "6      8911164         B  11.890  17.36   76.20   435.6  0.12250  0.07210   \n",
            "7       893548         B  13.050  13.84   82.71   530.6  0.08352  0.03735   \n",
            "8       867739         M  18.450  21.91  120.20  1075.0  0.09430  0.09709   \n",
            "9       857374         B  11.940  18.24   75.71   437.6  0.08261  0.04751   \n",
            "10      853201         M  17.570  15.05  115.00   955.1  0.09847  0.11570   \n",
            "11      857373         B  13.640  16.34   87.21   571.8  0.07685  0.06059   \n",
            "12       86561         B  13.850  17.21   88.44   588.7  0.08785  0.06136   \n",
            "13      899147         B  11.950  14.96   77.23   426.7  0.11580  0.12060   \n",
            "14      859464         B   9.465  21.01   60.11   269.4  0.10440  0.07773   \n",
            "15     8911800         B  13.590  17.84   86.24   572.3  0.07948  0.04052   \n",
            "16      871641         B  11.080  14.71   70.21   372.7  0.10060  0.05743   \n",
            "17      922577         B  10.320  16.35   65.31   324.9  0.09434  0.04994   \n",
            "18      897604         B  12.990  14.23   84.08   514.3  0.09462  0.09965   \n",
            "19    88119002         M  19.530  32.47  128.00  1223.0  0.08420  0.11300   \n",
            "20      925277         B  14.590  22.68   96.39   657.1  0.08473  0.13300   \n",
            "21     8810436         B  15.270  12.91   98.17   725.5  0.08182  0.06230   \n",
            "22      908489         M  13.980  19.62   91.12   599.5  0.10600  0.11330   \n",
            "23      873701         M  15.700  20.31  101.20   766.6  0.09597  0.08799   \n",
            "24      915940         B  14.580  13.66   94.29   658.8  0.09832  0.08918   \n",
            "25      869476         B  11.900  14.65   78.11   432.8  0.11520  0.12960   \n",
            "26      906539         B  11.570  19.04   74.20   409.7  0.08546  0.07722   \n",
            "27      862548         M  14.420  19.77   94.48   642.5  0.09752  0.11410   \n",
            "28      868999         B   9.738  11.97   61.24   288.5  0.09250  0.04102   \n",
            "29      859196         B   9.173  13.86   59.20   260.9  0.07721  0.08751   \n",
            "..         ...       ...     ...    ...     ...     ...      ...      ...   \n",
            "90      855138         M  13.480  20.82   88.40   559.2  0.10160  0.12550   \n",
            "91        8670         M  15.460  19.48  101.70   748.9  0.10920  0.12230   \n",
            "92      925236         B   9.423  27.88   59.26   271.3  0.08123  0.04971   \n",
            "93      901288         M  20.640  17.35  134.80  1335.0  0.09446  0.10760   \n",
            "94      854002         M  19.270  26.47  127.90  1162.0  0.09401  0.17190   \n",
            "95     8611555         M  25.220  24.91  171.50  1878.0  0.10630  0.26650   \n",
            "96      873593         M  21.090  26.57  142.70  1311.0  0.11410  0.28320   \n",
            "97      891703         B  11.850  17.46   75.54   432.7  0.08372  0.05642   \n",
            "98      925311         B  11.200  29.37   70.67   386.0  0.07449  0.03558   \n",
            "99       89813         B  14.420  16.54   94.15   641.2  0.09751  0.11390   \n",
            "0       894047         B   8.597  18.60   54.09   221.2  0.10740  0.05847   \n",
            "1       892189         M  11.760  18.14   75.00   431.1  0.09968  0.05914   \n",
            "2      8810528         B  11.840  18.94   75.51   428.0  0.08871  0.06900   \n",
            "3       905978         B   9.405  21.70   59.60   271.2  0.10440  0.06159   \n",
            "4    871001502         B   8.219  20.70   53.27   203.9  0.09405  0.13050   \n",
            "5        87880         M  13.810  23.75   91.56   597.8  0.13230  0.17680   \n",
            "6       882488         B   9.567  15.91   60.21   279.6  0.08464  0.04087   \n",
            "7    911296202         M  27.420  26.27  186.90  2501.0  0.10840  0.19880   \n",
            "8       861648         B  14.620  24.02   94.57   662.7  0.08974  0.08606   \n",
            "9       895100         M  20.340  21.51  135.90  1264.0  0.11700  0.18750   \n",
            "10      853612         M  11.840  18.70   77.93   440.6  0.11090  0.15160   \n",
            "11     8510653         B  13.080  15.71   85.63   520.0  0.10750  0.12700   \n",
            "12    88466802         B  10.650  25.22   68.01   347.0  0.09657  0.07234   \n",
            "13      911150         B  14.530  19.34   94.25   659.7  0.08388  0.07800   \n",
            "14     9110944         B  14.800  17.66   95.88   674.8  0.09179  0.08890   \n",
            "15     9113156         B  14.400  26.99   92.25   646.1  0.06995  0.05223   \n",
            "16      859711         B   8.888  14.64   58.79   244.0  0.09783  0.15310   \n",
            "17     9013579         B  13.460  28.21   85.89   562.1  0.07517  0.04726   \n",
            "18    86973701         B  14.950  18.77   97.84   689.5  0.08138  0.11670   \n",
            "19    85638502         M  13.170  21.81   85.42   531.5  0.09714  0.10470   \n",
            "\n",
            "          f7        f8  ...     f21    f22     f23     f24      f25      f26  \\\n",
            "0   0.020870  0.026520  ...  14.910  19.31   96.53   688.9  0.10340  0.10170   \n",
            "1   0.198000  0.104300  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500   \n",
            "2   0.111500  0.033900  ...  13.900  19.69   92.12   595.6  0.09926  0.23170   \n",
            "3   0.019230  0.019680  ...  11.380  15.65   73.23   394.5  0.13430  0.16500   \n",
            "4   0.018000  0.012560  ...  14.500  28.46   95.29   648.3  0.11180  0.16460   \n",
            "5   0.039960  0.012820  ...   9.845  25.05   62.86   295.8  0.11030  0.08298   \n",
            "6   0.059290  0.074040  ...  12.400  18.99   79.46   472.4  0.13590  0.08368   \n",
            "7   0.004559  0.008829  ...  14.730  17.40   93.96   672.4  0.10160  0.05847   \n",
            "8   0.115300  0.068470  ...  22.520  31.39  145.60  1590.0  0.14650  0.22750   \n",
            "9   0.019720  0.013490  ...  13.100  21.33   83.67   527.2  0.11440  0.08906   \n",
            "10  0.098750  0.079530  ...  20.010  19.52  134.90  1227.0  0.12550  0.28120   \n",
            "11  0.018570  0.017230  ...  14.670  23.19   96.08   656.7  0.10890  0.15820   \n",
            "12  0.014200  0.011410  ...  15.490  23.58  100.30   725.9  0.11570  0.13500   \n",
            "13  0.011710  0.017870  ...  12.810  17.72   83.09   496.2  0.12930  0.18850   \n",
            "14  0.021720  0.015040  ...  10.410  31.56   67.03   330.7  0.15480  0.16640   \n",
            "15  0.019970  0.012380  ...  15.500  26.10   98.91   739.1  0.10500  0.07622   \n",
            "16  0.023630  0.025830  ...  11.350  16.82   72.01   396.5  0.12160  0.08240   \n",
            "17  0.010120  0.005495  ...  11.250  21.77   71.12   384.9  0.12850  0.08842   \n",
            "18  0.037380  0.020980  ...  13.720  16.91   87.38   576.0  0.11420  0.19750   \n",
            "19  0.114500  0.066370  ...  27.900  45.41  180.20  2477.0  0.14080  0.40970   \n",
            "20  0.102900  0.037360  ...  15.480  27.27  105.90   733.5  0.10260  0.31710   \n",
            "21  0.058920  0.031570  ...  17.380  15.92  113.70   932.7  0.12220  0.21860   \n",
            "22  0.112600  0.064630  ...  17.040  30.80  113.90   869.3  0.16130  0.35680   \n",
            "23  0.065930  0.051890  ...  20.110  32.82  129.30  1269.0  0.14140  0.35470   \n",
            "24  0.082220  0.043490  ...  16.760  17.24  108.50   862.0  0.12230  0.19280   \n",
            "25  0.037100  0.030030  ...  13.150  16.51   86.26   509.6  0.14240  0.25170   \n",
            "26  0.054850  0.014280  ...  13.070  26.98   86.43   520.5  0.12490  0.19370   \n",
            "27  0.093880  0.058390  ...  16.330  30.86  109.50   826.4  0.14310  0.30260   \n",
            "28  0.000000  0.000000  ...  10.620  14.10   66.53   342.9  0.12340  0.07204   \n",
            "29  0.059880  0.021800  ...  10.010  19.23   65.59   310.1  0.09836  0.16780   \n",
            "..       ...       ...  ...     ...    ...     ...     ...      ...      ...   \n",
            "90  0.106300  0.054390  ...  15.530  26.02  107.30   740.4  0.16100  0.42250   \n",
            "91  0.146600  0.080870  ...  19.260  26.00  124.90  1156.0  0.15460  0.23940   \n",
            "92  0.000000  0.000000  ...  10.490  34.24   66.50   330.6  0.10730  0.07158   \n",
            "93  0.152700  0.089410  ...  25.370  23.17  166.80  1946.0  0.15620  0.30550   \n",
            "94  0.165700  0.075930  ...  24.150  30.90  161.40  1813.0  0.15090  0.65900   \n",
            "95  0.333900  0.184500  ...  30.000  33.62  211.70  2562.0  0.15730  0.60760   \n",
            "96  0.248700  0.149600  ...  26.680  33.48  176.50  2089.0  0.14910  0.75840   \n",
            "97  0.026880  0.022800  ...  13.060  25.75   84.35   517.8  0.13690  0.17580   \n",
            "98  0.000000  0.000000  ...  11.920  38.30   75.19   439.6  0.09267  0.05494   \n",
            "99  0.080070  0.042230  ...  16.670  21.51  111.40   862.1  0.12940  0.33710   \n",
            "0   0.000000  0.000000  ...   8.952  22.44   56.65   240.1  0.13470  0.07767   \n",
            "1   0.026850  0.035150  ...  13.360  23.39   85.10   553.6  0.11370  0.07974   \n",
            "2   0.026690  0.013930  ...  13.300  24.99   85.22   546.3  0.12800  0.18800   \n",
            "3   0.020470  0.012570  ...  10.850  31.24   68.73   359.4  0.15260  0.11930   \n",
            "4   0.132100  0.021680  ...   9.092  29.72   58.08   249.8  0.16300  0.43100   \n",
            "5   0.155800  0.091760  ...  19.200  41.85  128.50  1153.0  0.22260  0.52090   \n",
            "6   0.016520  0.016670  ...  10.510  19.16   65.74   335.9  0.15040  0.09515   \n",
            "7   0.363500  0.168900  ...  36.040  31.37  251.20  4254.0  0.13570  0.42560   \n",
            "8   0.031020  0.029570  ...  16.110  29.11  102.90   803.7  0.11150  0.17660   \n",
            "9   0.256500  0.150400  ...  25.300  31.86  171.10  1938.0  0.15920  0.44920   \n",
            "10  0.121800  0.051820  ...  16.820  28.12  119.40   888.7  0.16370  0.57750   \n",
            "11  0.045680  0.031100  ...  14.500  20.49   96.09   630.5  0.13120  0.27760   \n",
            "12  0.023790  0.016150  ...  12.250  35.19   77.98   455.7  0.14990  0.13980   \n",
            "13  0.088170  0.029250  ...  16.300  28.39  108.10   830.5  0.10890  0.26490   \n",
            "14  0.040690  0.022600  ...  16.430  22.74  105.90   829.5  0.12260  0.18810   \n",
            "15  0.034760  0.017370  ...     NaN  31.98  100.40   734.6  0.10170  0.14600   \n",
            "16  0.086060  0.028720  ...   9.733  15.67   62.56   284.4  0.12070  0.24360   \n",
            "17  0.012710  0.011170  ...  14.690  35.63   97.11   680.6  0.11080  0.14570   \n",
            "18  0.090500  0.035620  ...  16.250  25.47  107.10   809.7  0.09970  0.25210   \n",
            "19  0.082590  0.052520  ...  16.230  29.89  105.50   740.7  0.15030  0.39040   \n",
            "\n",
            "        f27      f28     f29      f30  \n",
            "0   0.06260  0.08216  0.2136  0.06710  \n",
            "1   0.40000  0.16250  0.2364  0.07678  \n",
            "2   0.33440  0.10170  0.1999  0.07127  \n",
            "3   0.08615  0.06696  0.2937  0.07722  \n",
            "4   0.07698  0.04195  0.2687  0.07429  \n",
            "5   0.07993  0.02564  0.2435  0.07393  \n",
            "6   0.07153  0.08946  0.2220  0.06033  \n",
            "7   0.01824  0.03532  0.2107  0.06580  \n",
            "8   0.39650  0.13790  0.3109  0.07610  \n",
            "9   0.09203  0.06296  0.2785  0.07408  \n",
            "10  0.24890  0.14560  0.2756  0.07919  \n",
            "11  0.10500  0.08586  0.2346  0.08025  \n",
            "12  0.08115  0.05104  0.2364  0.07182  \n",
            "13  0.03122  0.04766  0.3124  0.07590  \n",
            "14  0.09412  0.06517  0.2878  0.09211  \n",
            "15  0.10600  0.05185  0.2335  0.06263  \n",
            "16  0.03938  0.04306  0.1902  0.07313  \n",
            "17  0.04384  0.02381  0.2681  0.07399  \n",
            "18  0.14500  0.05850  0.2432  0.10090  \n",
            "19  0.39950  0.16250  0.2713  0.07568  \n",
            "20  0.36620  0.11050  0.2258  0.08004  \n",
            "21  0.29620  0.10350  0.2320  0.07474  \n",
            "22  0.40690  0.18270  0.3179  0.10550  \n",
            "23  0.29020  0.15410  0.3437  0.08631  \n",
            "24  0.24920  0.09186  0.2626  0.07048  \n",
            "25  0.09420  0.06042  0.2727  0.10360  \n",
            "26  0.25600  0.06664  0.3035  0.08284  \n",
            "27  0.31940  0.15650  0.2718  0.09353  \n",
            "28  0.00000  0.00000  0.3105  0.08151  \n",
            "29  0.13970  0.05087  0.3282  0.08490  \n",
            "..      ...      ...     ...      ...  \n",
            "90  0.50300  0.22580  0.2807  0.10710  \n",
            "91  0.37910  0.15140  0.2837  0.08019  \n",
            "92  0.00000  0.00000  0.2475  0.06969  \n",
            "93  0.41590  0.21120  0.2689  0.07055  \n",
            "94  0.60910  0.17850  0.3672  0.11230  \n",
            "95  0.64760  0.28670  0.2355  0.10510  \n",
            "96  0.67800  0.29030  0.4098  0.12840  \n",
            "97  0.13160  0.09140  0.3101  0.07007  \n",
            "98  0.00000  0.00000  0.1566  0.05905  \n",
            "99  0.37550  0.14140  0.3053  0.08764  \n",
            "0   0.00000  0.00000  0.3142  0.08116  \n",
            "1   0.06120  0.07160  0.1978  0.06915  \n",
            "2   0.14710  0.06913  0.2535  0.07993  \n",
            "3   0.06141  0.03770  0.2872  0.08304  \n",
            "4   0.53810  0.07879  0.3322  0.14860  \n",
            "5   0.46460  0.20130  0.4432  0.10860  \n",
            "6   0.07161  0.07222  0.2757  0.08178  \n",
            "7   0.68330  0.26250  0.2641  0.07427  \n",
            "8   0.09189  0.06946  0.2522  0.07246  \n",
            "9   0.53440  0.26850  0.5558  0.10240  \n",
            "10  0.69560  0.15460  0.4761  0.14020  \n",
            "11  0.18900  0.07283  0.3184  0.08183  \n",
            "12  0.11250  0.06136  0.3409  0.08147  \n",
            "13  0.37790  0.09594  0.2471  0.07463  \n",
            "14  0.20600  0.08308  0.3600  0.07285  \n",
            "15  0.14720  0.05563  0.2345  0.06464  \n",
            "16  0.14340  0.04786  0.2254  0.10840  \n",
            "17  0.07934  0.05781  0.2694  0.07061  \n",
            "18  0.25000  0.08405  0.2852  0.09218  \n",
            "19  0.37280  0.16070  0.3693  0.09618  \n",
            "\n",
            "[120 rows x 32 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3b7R1oEpSeF",
        "colab_type": "code",
        "outputId": "b4b5fbc0-c488-4bf3-88e2-c7c7f7932640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We can see from the dataset all the feature columns starts with \"f\"\n",
        "features = dataset.filter(regex='[f]', axis=1)\n",
        "number_of_examples, number_of_features = features.shape\n",
        "print(f\"The number of features in the dataset are: {number_of_features}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of features in the dataset are: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFxh84MpneX",
        "colab_type": "code",
        "outputId": "1bcc7f90-d842-4da9-8042-4df1e6b397f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "features.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>...</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.02</td>\n",
              "      <td>15.66</td>\n",
              "      <td>89.59</td>\n",
              "      <td>606.5</td>\n",
              "      <td>0.07966</td>\n",
              "      <td>0.05581</td>\n",
              "      <td>0.02087</td>\n",
              "      <td>0.02652</td>\n",
              "      <td>0.1589</td>\n",
              "      <td>0.05586</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>19.31</td>\n",
              "      <td>96.53</td>\n",
              "      <td>688.9</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.1017</td>\n",
              "      <td>0.06260</td>\n",
              "      <td>0.08216</td>\n",
              "      <td>0.2136</td>\n",
              "      <td>0.06710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.40000</td>\n",
              "      <td>0.16250</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.89</td>\n",
              "      <td>15.70</td>\n",
              "      <td>84.08</td>\n",
              "      <td>516.6</td>\n",
              "      <td>0.07818</td>\n",
              "      <td>0.09580</td>\n",
              "      <td>0.11150</td>\n",
              "      <td>0.03390</td>\n",
              "      <td>0.1432</td>\n",
              "      <td>0.05935</td>\n",
              "      <td>...</td>\n",
              "      <td>13.90</td>\n",
              "      <td>19.69</td>\n",
              "      <td>92.12</td>\n",
              "      <td>595.6</td>\n",
              "      <td>0.09926</td>\n",
              "      <td>0.2317</td>\n",
              "      <td>0.33440</td>\n",
              "      <td>0.10170</td>\n",
              "      <td>0.1999</td>\n",
              "      <td>0.07127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.26</td>\n",
              "      <td>12.22</td>\n",
              "      <td>65.75</td>\n",
              "      <td>321.6</td>\n",
              "      <td>0.09996</td>\n",
              "      <td>0.07542</td>\n",
              "      <td>0.01923</td>\n",
              "      <td>0.01968</td>\n",
              "      <td>0.1800</td>\n",
              "      <td>0.06569</td>\n",
              "      <td>...</td>\n",
              "      <td>11.38</td>\n",
              "      <td>15.65</td>\n",
              "      <td>73.23</td>\n",
              "      <td>394.5</td>\n",
              "      <td>0.13430</td>\n",
              "      <td>0.1650</td>\n",
              "      <td>0.08615</td>\n",
              "      <td>0.06696</td>\n",
              "      <td>0.2937</td>\n",
              "      <td>0.07722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.16</td>\n",
              "      <td>20.54</td>\n",
              "      <td>84.06</td>\n",
              "      <td>538.7</td>\n",
              "      <td>0.07335</td>\n",
              "      <td>0.05275</td>\n",
              "      <td>0.01800</td>\n",
              "      <td>0.01256</td>\n",
              "      <td>0.1713</td>\n",
              "      <td>0.05888</td>\n",
              "      <td>...</td>\n",
              "      <td>14.50</td>\n",
              "      <td>28.46</td>\n",
              "      <td>95.29</td>\n",
              "      <td>648.3</td>\n",
              "      <td>0.11180</td>\n",
              "      <td>0.1646</td>\n",
              "      <td>0.07698</td>\n",
              "      <td>0.04195</td>\n",
              "      <td>0.2687</td>\n",
              "      <td>0.07429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      f1     f2      f3      f4       f5       f6       f7       f8      f9  \\\n",
              "0  14.02  15.66   89.59   606.5  0.07966  0.05581  0.02087  0.02652  0.1589   \n",
              "1  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430  0.1809   \n",
              "2  12.89  15.70   84.08   516.6  0.07818  0.09580  0.11150  0.03390  0.1432   \n",
              "3  10.26  12.22   65.75   321.6  0.09996  0.07542  0.01923  0.01968  0.1800   \n",
              "4  13.16  20.54   84.06   538.7  0.07335  0.05275  0.01800  0.01256  0.1713   \n",
              "\n",
              "       f10  ...    f21    f22     f23     f24      f25     f26      f27  \\\n",
              "0  0.05586  ...  14.91  19.31   96.53   688.9  0.10340  0.1017  0.06260   \n",
              "1  0.05883  ...  22.54  16.67  152.20  1575.0  0.13740  0.2050  0.40000   \n",
              "2  0.05935  ...  13.90  19.69   92.12   595.6  0.09926  0.2317  0.33440   \n",
              "3  0.06569  ...  11.38  15.65   73.23   394.5  0.13430  0.1650  0.08615   \n",
              "4  0.05888  ...  14.50  28.46   95.29   648.3  0.11180  0.1646  0.07698   \n",
              "\n",
              "       f28     f29      f30  \n",
              "0  0.08216  0.2136  0.06710  \n",
              "1  0.16250  0.2364  0.07678  \n",
              "2  0.10170  0.1999  0.07127  \n",
              "3  0.06696  0.2937  0.07722  \n",
              "4  0.04195  0.2687  0.07429  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzBASxFAr5Im",
        "colab_type": "code",
        "outputId": "4a0badf1-8fe9-4dfb-d52d-46bed64c50ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3247
        }
      },
      "source": [
        "X=features\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        f1     f2      f3      f4       f5       f6        f7        f8  \\\n",
            "0   14.020  15.66   89.59   606.5  0.07966  0.05581  0.020870  0.026520   \n",
            "1   20.290  14.34  135.10  1297.0  0.10030  0.13280  0.198000  0.104300   \n",
            "2   12.890  15.70   84.08   516.6  0.07818  0.09580  0.111500  0.033900   \n",
            "3   10.260  12.22   65.75   321.6  0.09996  0.07542  0.019230  0.019680   \n",
            "4   13.160  20.54   84.06   538.7  0.07335  0.05275  0.018000  0.012560   \n",
            "5    9.333  21.94   59.01   264.0  0.09240  0.05605  0.039960  0.012820   \n",
            "6   11.890  17.36   76.20   435.6  0.12250  0.07210  0.059290  0.074040   \n",
            "7   13.050  13.84   82.71   530.6  0.08352  0.03735  0.004559  0.008829   \n",
            "8   18.450  21.91  120.20  1075.0  0.09430  0.09709  0.115300  0.068470   \n",
            "9   11.940  18.24   75.71   437.6  0.08261  0.04751  0.019720  0.013490   \n",
            "10  17.570  15.05  115.00   955.1  0.09847  0.11570  0.098750  0.079530   \n",
            "11  13.640  16.34   87.21   571.8  0.07685  0.06059  0.018570  0.017230   \n",
            "12  13.850  17.21   88.44   588.7  0.08785  0.06136  0.014200  0.011410   \n",
            "13  11.950  14.96   77.23   426.7  0.11580  0.12060  0.011710  0.017870   \n",
            "14   9.465  21.01   60.11   269.4  0.10440  0.07773  0.021720  0.015040   \n",
            "15  13.590  17.84   86.24   572.3  0.07948  0.04052  0.019970  0.012380   \n",
            "16  11.080  14.71   70.21   372.7  0.10060  0.05743  0.023630  0.025830   \n",
            "17  10.320  16.35   65.31   324.9  0.09434  0.04994  0.010120  0.005495   \n",
            "18  12.990  14.23   84.08   514.3  0.09462  0.09965  0.037380  0.020980   \n",
            "19  19.530  32.47  128.00  1223.0  0.08420  0.11300  0.114500  0.066370   \n",
            "20  14.590  22.68   96.39   657.1  0.08473  0.13300  0.102900  0.037360   \n",
            "21  15.270  12.91   98.17   725.5  0.08182  0.06230  0.058920  0.031570   \n",
            "22  13.980  19.62   91.12   599.5  0.10600  0.11330  0.112600  0.064630   \n",
            "23  15.700  20.31  101.20   766.6  0.09597  0.08799  0.065930  0.051890   \n",
            "24  14.580  13.66   94.29   658.8  0.09832  0.08918  0.082220  0.043490   \n",
            "25  11.900  14.65   78.11   432.8  0.11520  0.12960  0.037100  0.030030   \n",
            "26  11.570  19.04   74.20   409.7  0.08546  0.07722  0.054850  0.014280   \n",
            "27  14.420  19.77   94.48   642.5  0.09752  0.11410  0.093880  0.058390   \n",
            "28   9.738  11.97   61.24   288.5  0.09250  0.04102  0.000000  0.000000   \n",
            "29   9.173  13.86   59.20   260.9  0.07721  0.08751  0.059880  0.021800   \n",
            "..     ...    ...     ...     ...      ...      ...       ...       ...   \n",
            "90  13.480  20.82   88.40   559.2  0.10160  0.12550  0.106300  0.054390   \n",
            "91  15.460  19.48  101.70   748.9  0.10920  0.12230  0.146600  0.080870   \n",
            "92   9.423  27.88   59.26   271.3  0.08123  0.04971  0.000000  0.000000   \n",
            "93  20.640  17.35  134.80  1335.0  0.09446  0.10760  0.152700  0.089410   \n",
            "94  19.270  26.47  127.90  1162.0  0.09401  0.17190  0.165700  0.075930   \n",
            "95  25.220  24.91  171.50  1878.0  0.10630  0.26650  0.333900  0.184500   \n",
            "96  21.090  26.57  142.70  1311.0  0.11410  0.28320  0.248700  0.149600   \n",
            "97  11.850  17.46   75.54   432.7  0.08372  0.05642  0.026880  0.022800   \n",
            "98  11.200  29.37   70.67   386.0  0.07449  0.03558  0.000000  0.000000   \n",
            "99  14.420  16.54   94.15   641.2  0.09751  0.11390  0.080070  0.042230   \n",
            "0    8.597  18.60   54.09   221.2  0.10740  0.05847  0.000000  0.000000   \n",
            "1   11.760  18.14   75.00   431.1  0.09968  0.05914  0.026850  0.035150   \n",
            "2   11.840  18.94   75.51   428.0  0.08871  0.06900  0.026690  0.013930   \n",
            "3    9.405  21.70   59.60   271.2  0.10440  0.06159  0.020470  0.012570   \n",
            "4    8.219  20.70   53.27   203.9  0.09405  0.13050  0.132100  0.021680   \n",
            "5   13.810  23.75   91.56   597.8  0.13230  0.17680  0.155800  0.091760   \n",
            "6    9.567  15.91   60.21   279.6  0.08464  0.04087  0.016520  0.016670   \n",
            "7   27.420  26.27  186.90  2501.0  0.10840  0.19880  0.363500  0.168900   \n",
            "8   14.620  24.02   94.57   662.7  0.08974  0.08606  0.031020  0.029570   \n",
            "9   20.340  21.51  135.90  1264.0  0.11700  0.18750  0.256500  0.150400   \n",
            "10  11.840  18.70   77.93   440.6  0.11090  0.15160  0.121800  0.051820   \n",
            "11  13.080  15.71   85.63   520.0  0.10750  0.12700  0.045680  0.031100   \n",
            "12  10.650  25.22   68.01   347.0  0.09657  0.07234  0.023790  0.016150   \n",
            "13  14.530  19.34   94.25   659.7  0.08388  0.07800  0.088170  0.029250   \n",
            "14  14.800  17.66   95.88   674.8  0.09179  0.08890  0.040690  0.022600   \n",
            "15  14.400  26.99   92.25   646.1  0.06995  0.05223  0.034760  0.017370   \n",
            "16   8.888  14.64   58.79   244.0  0.09783  0.15310  0.086060  0.028720   \n",
            "17  13.460  28.21   85.89   562.1  0.07517  0.04726  0.012710  0.011170   \n",
            "18  14.950  18.77   97.84   689.5  0.08138  0.11670  0.090500  0.035620   \n",
            "19  13.170  21.81   85.42   531.5  0.09714  0.10470  0.082590  0.052520   \n",
            "\n",
            "        f9      f10  ...     f21    f22     f23     f24      f25      f26  \\\n",
            "0   0.1589  0.05586  ...  14.910  19.31   96.53   688.9  0.10340  0.10170   \n",
            "1   0.1809  0.05883  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500   \n",
            "2   0.1432  0.05935  ...  13.900  19.69   92.12   595.6  0.09926  0.23170   \n",
            "3   0.1800  0.06569  ...  11.380  15.65   73.23   394.5  0.13430  0.16500   \n",
            "4   0.1713  0.05888  ...  14.500  28.46   95.29   648.3  0.11180  0.16460   \n",
            "5   0.1692  0.06576  ...   9.845  25.05   62.86   295.8  0.11030  0.08298   \n",
            "6   0.2015  0.05875  ...  12.400  18.99   79.46   472.4  0.13590  0.08368   \n",
            "7   0.1453  0.05518  ...  14.730  17.40   93.96   672.4  0.10160  0.05847   \n",
            "8   0.1692  0.05727  ...  22.520  31.39  145.60  1590.0  0.14650  0.22750   \n",
            "9   0.1868  0.06110  ...  13.100  21.33   83.67   527.2  0.11440  0.08906   \n",
            "10  0.1739  0.06149  ...  20.010  19.52  134.90  1227.0  0.12550  0.28120   \n",
            "11  0.1353  0.05953  ...  14.670  23.19   96.08   656.7  0.10890  0.15820   \n",
            "12  0.1614  0.05890  ...  15.490  23.58  100.30   725.9  0.11570  0.13500   \n",
            "13  0.2459  0.06581  ...  12.810  17.72   83.09   496.2  0.12930  0.18850   \n",
            "14  0.1717  0.06899  ...  10.410  31.56   67.03   330.7  0.15480  0.16640   \n",
            "15  0.1573  0.05520  ...  15.500  26.10   98.91   739.1  0.10500  0.07622   \n",
            "16  0.1566  0.06669  ...  11.350  16.82   72.01   396.5  0.12160  0.08240   \n",
            "17  0.1885  0.06201  ...  11.250  21.77   71.12   384.9  0.12850  0.08842   \n",
            "18  0.1652  0.07238  ...  13.720  16.91   87.38   576.0  0.11420  0.19750   \n",
            "19  0.1428  0.05313  ...  27.900  45.41  180.20  2477.0  0.14080  0.40970   \n",
            "20  0.1454  0.06147  ...  15.480  27.27  105.90   733.5  0.10260  0.31710   \n",
            "21  0.1359  0.05526  ...  17.380  15.92  113.70   932.7  0.12220  0.21860   \n",
            "22  0.1669  0.06544  ...  17.040  30.80  113.90   869.3  0.16130  0.35680   \n",
            "23  0.1618  0.05549  ...  20.110  32.82  129.30  1269.0  0.14140  0.35470   \n",
            "24  0.1739  0.05640  ...  16.760  17.24  108.50   862.0  0.12230  0.19280   \n",
            "25  0.1995  0.07839  ...  13.150  16.51   86.26   509.6  0.14240  0.25170   \n",
            "26  0.2031  0.06267  ...  13.070  26.98   86.43   520.5  0.12490  0.19370   \n",
            "27  0.1879  0.06390  ...  16.330  30.86  109.50   826.4  0.14310  0.30260   \n",
            "28  0.1903  0.06422  ...  10.620  14.10   66.53   342.9  0.12340  0.07204   \n",
            "29  0.2341  0.06963  ...  10.010  19.23   65.59   310.1  0.09836  0.16780   \n",
            "..     ...      ...  ...     ...    ...     ...     ...      ...      ...   \n",
            "90  0.1720  0.06419  ...  15.530  26.02  107.30   740.4  0.16100  0.42250   \n",
            "91  0.1931  0.05796  ...  19.260  26.00  124.90  1156.0  0.15460  0.23940   \n",
            "92  0.1742  0.06059  ...  10.490  34.24   66.50   330.6  0.10730  0.07158   \n",
            "93  0.1571  0.05478  ...  25.370  23.17  166.80  1946.0  0.15620  0.30550   \n",
            "94  0.1853  0.06261  ...  24.150  30.90  161.40  1813.0  0.15090  0.65900   \n",
            "95  0.1829  0.06782  ...  30.000  33.62  211.70  2562.0  0.15730  0.60760   \n",
            "96  0.2395  0.07398  ...  26.680  33.48  176.50  2089.0  0.14910  0.75840   \n",
            "97  0.1875  0.05715  ...  13.060  25.75   84.35   517.8  0.13690  0.17580   \n",
            "98  0.1060  0.05502  ...  11.920  38.30   75.19   439.6  0.09267  0.05494   \n",
            "99  0.1912  0.06412  ...  16.670  21.51  111.40   862.1  0.12940  0.33710   \n",
            "0   0.2163  0.07359  ...   8.952  22.44   56.65   240.1  0.13470  0.07767   \n",
            "1   0.1619  0.06287  ...  13.360  23.39   85.10   553.6  0.11370  0.07974   \n",
            "2   0.1533  0.06057  ...  13.300  24.99   85.22   546.3  0.12800  0.18800   \n",
            "3   0.2025  0.06601  ...  10.850  31.24   68.73   359.4  0.15260  0.11930   \n",
            "4   0.2222  0.08261  ...   9.092  29.72   58.08   249.8  0.16300  0.43100   \n",
            "5   0.2251  0.07421  ...  19.200  41.85  128.50  1153.0  0.22260  0.52090   \n",
            "6   0.1551  0.06403  ...  10.510  19.16   65.74   335.9  0.15040  0.09515   \n",
            "7   0.2061  0.05623  ...  36.040  31.37  251.20  4254.0  0.13570  0.42560   \n",
            "8   0.1685  0.05866  ...  16.110  29.11  102.90   803.7  0.11150  0.17660   \n",
            "9   0.2569  0.06670  ...  25.300  31.86  171.10  1938.0  0.15920  0.44920   \n",
            "10  0.2301  0.07799  ...  16.820  28.12  119.40   888.7  0.16370  0.57750   \n",
            "11  0.1967  0.06811  ...  14.500  20.49   96.09   630.5  0.13120  0.27760   \n",
            "12  0.1897  0.06329  ...  12.250  35.19   77.98   455.7  0.14990  0.13980   \n",
            "13  0.1473  0.05746  ...  16.300  28.39  108.10   830.5  0.10890  0.26490   \n",
            "14  0.1893  0.05886  ...  16.430  22.74  105.90   829.5  0.12260  0.18810   \n",
            "15  0.1707  0.05433  ...     NaN  31.98  100.40   734.6  0.10170  0.14600   \n",
            "16  0.1902  0.08980  ...   9.733  15.67   62.56   284.4  0.12070  0.24360   \n",
            "17  0.1421  0.05763  ...  14.690  35.63   97.11   680.6  0.11080  0.14570   \n",
            "18  0.1744  0.06493  ...  16.250  25.47  107.10   809.7  0.09970  0.25210   \n",
            "19  0.1746  0.06177  ...  16.230  29.89  105.50   740.7  0.15030  0.39040   \n",
            "\n",
            "        f27      f28     f29      f30  \n",
            "0   0.06260  0.08216  0.2136  0.06710  \n",
            "1   0.40000  0.16250  0.2364  0.07678  \n",
            "2   0.33440  0.10170  0.1999  0.07127  \n",
            "3   0.08615  0.06696  0.2937  0.07722  \n",
            "4   0.07698  0.04195  0.2687  0.07429  \n",
            "5   0.07993  0.02564  0.2435  0.07393  \n",
            "6   0.07153  0.08946  0.2220  0.06033  \n",
            "7   0.01824  0.03532  0.2107  0.06580  \n",
            "8   0.39650  0.13790  0.3109  0.07610  \n",
            "9   0.09203  0.06296  0.2785  0.07408  \n",
            "10  0.24890  0.14560  0.2756  0.07919  \n",
            "11  0.10500  0.08586  0.2346  0.08025  \n",
            "12  0.08115  0.05104  0.2364  0.07182  \n",
            "13  0.03122  0.04766  0.3124  0.07590  \n",
            "14  0.09412  0.06517  0.2878  0.09211  \n",
            "15  0.10600  0.05185  0.2335  0.06263  \n",
            "16  0.03938  0.04306  0.1902  0.07313  \n",
            "17  0.04384  0.02381  0.2681  0.07399  \n",
            "18  0.14500  0.05850  0.2432  0.10090  \n",
            "19  0.39950  0.16250  0.2713  0.07568  \n",
            "20  0.36620  0.11050  0.2258  0.08004  \n",
            "21  0.29620  0.10350  0.2320  0.07474  \n",
            "22  0.40690  0.18270  0.3179  0.10550  \n",
            "23  0.29020  0.15410  0.3437  0.08631  \n",
            "24  0.24920  0.09186  0.2626  0.07048  \n",
            "25  0.09420  0.06042  0.2727  0.10360  \n",
            "26  0.25600  0.06664  0.3035  0.08284  \n",
            "27  0.31940  0.15650  0.2718  0.09353  \n",
            "28  0.00000  0.00000  0.3105  0.08151  \n",
            "29  0.13970  0.05087  0.3282  0.08490  \n",
            "..      ...      ...     ...      ...  \n",
            "90  0.50300  0.22580  0.2807  0.10710  \n",
            "91  0.37910  0.15140  0.2837  0.08019  \n",
            "92  0.00000  0.00000  0.2475  0.06969  \n",
            "93  0.41590  0.21120  0.2689  0.07055  \n",
            "94  0.60910  0.17850  0.3672  0.11230  \n",
            "95  0.64760  0.28670  0.2355  0.10510  \n",
            "96  0.67800  0.29030  0.4098  0.12840  \n",
            "97  0.13160  0.09140  0.3101  0.07007  \n",
            "98  0.00000  0.00000  0.1566  0.05905  \n",
            "99  0.37550  0.14140  0.3053  0.08764  \n",
            "0   0.00000  0.00000  0.3142  0.08116  \n",
            "1   0.06120  0.07160  0.1978  0.06915  \n",
            "2   0.14710  0.06913  0.2535  0.07993  \n",
            "3   0.06141  0.03770  0.2872  0.08304  \n",
            "4   0.53810  0.07879  0.3322  0.14860  \n",
            "5   0.46460  0.20130  0.4432  0.10860  \n",
            "6   0.07161  0.07222  0.2757  0.08178  \n",
            "7   0.68330  0.26250  0.2641  0.07427  \n",
            "8   0.09189  0.06946  0.2522  0.07246  \n",
            "9   0.53440  0.26850  0.5558  0.10240  \n",
            "10  0.69560  0.15460  0.4761  0.14020  \n",
            "11  0.18900  0.07283  0.3184  0.08183  \n",
            "12  0.11250  0.06136  0.3409  0.08147  \n",
            "13  0.37790  0.09594  0.2471  0.07463  \n",
            "14  0.20600  0.08308  0.3600  0.07285  \n",
            "15  0.14720  0.05563  0.2345  0.06464  \n",
            "16  0.14340  0.04786  0.2254  0.10840  \n",
            "17  0.07934  0.05781  0.2694  0.07061  \n",
            "18  0.25000  0.08405  0.2852  0.09218  \n",
            "19  0.37280  0.16070  0.3693  0.09618  \n",
            "\n",
            "[120 rows x 30 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRofspCirATC",
        "colab_type": "code",
        "outputId": "723ff6a3-7e2b-4d6d-b3c6-a80da5d87ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        }
      },
      "source": [
        "y=pd.DataFrame(dataset.iloc[:,1])\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Diagnosis\n",
            "0          B\n",
            "1          M\n",
            "2          B\n",
            "3          B\n",
            "4          B\n",
            "5          B\n",
            "6          B\n",
            "7          B\n",
            "8          M\n",
            "9          B\n",
            "10         M\n",
            "11         B\n",
            "12         B\n",
            "13         B\n",
            "14         B\n",
            "15         B\n",
            "16         B\n",
            "17         B\n",
            "18         B\n",
            "19         M\n",
            "20         B\n",
            "21         B\n",
            "22         M\n",
            "23         M\n",
            "24         B\n",
            "25         B\n",
            "26         B\n",
            "27         M\n",
            "28         B\n",
            "29         B\n",
            "..       ...\n",
            "90         M\n",
            "91         M\n",
            "92         B\n",
            "93         M\n",
            "94         M\n",
            "95         M\n",
            "96         M\n",
            "97         B\n",
            "98         B\n",
            "99         B\n",
            "0          B\n",
            "1          M\n",
            "2          B\n",
            "3          B\n",
            "4          B\n",
            "5          M\n",
            "6          B\n",
            "7          M\n",
            "8          B\n",
            "9          M\n",
            "10         M\n",
            "11         B\n",
            "12         B\n",
            "13         B\n",
            "14         B\n",
            "15         B\n",
            "16         B\n",
            "17         B\n",
            "18         B\n",
            "19         M\n",
            "\n",
            "[120 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc5Gvf2JsSoQ",
        "colab_type": "code",
        "outputId": "265008cd-e30e-4c22-9049-6e9caa37085a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Encoding Categorical Data (Diagnosis)\n",
        "#Malignant(M)-1\n",
        "#Benign(B)-0\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_y=LabelEncoder()\n",
        "y=labelencoder_y.fit_transform(y)\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0\n",
            " 0 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0\n",
            " 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3vhXMegs0G_",
        "colab_type": "code",
        "outputId": "4e63cc95-1d12-4b97-86c3-b57eef3cdcd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "#Total Number of 1's and 0's in the training set\n",
        "import seaborn as sns\n",
        "sns.countplot(y,label=\"Count\").set_title('Training Set')\n",
        "\n",
        "(y == 0).astype(int).sum(axis=0)\n",
        "(y == 1).astype(int).sum(axis=0)\n",
        "\n",
        "print(f'Number of Zeroes in training label: {(y == 0).astype(int).sum(axis=0)}')\n",
        "print(f'Number of Ones in training label: {(y == 1).astype(int).sum(axis=0)}')\n",
        "print(f'Diagnosis column in training set is unbalanced')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Zeroes in training label: 72\n",
            "Number of Ones in training label: 48\n",
            "Diagnosis column in training set is unbalanced\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEhS4wwCSWrc/wOpE2ASh61ILgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1M0lzI0uBXp",
        "colab_type": "code",
        "outputId": "a3f7243b-7bc6-4650-9627-82d94d1b17fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "X.isnull().any()\n",
        "X.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "f1     0\n",
              "f2     0\n",
              "f3     0\n",
              "f4     0\n",
              "f5     0\n",
              "f6     0\n",
              "f7     0\n",
              "f8     0\n",
              "f9     0\n",
              "f10    0\n",
              "f11    0\n",
              "f12    0\n",
              "f13    0\n",
              "f14    0\n",
              "f15    0\n",
              "f16    0\n",
              "f17    0\n",
              "f18    0\n",
              "f19    0\n",
              "f20    0\n",
              "f21    3\n",
              "f22    0\n",
              "f23    0\n",
              "f24    0\n",
              "f25    0\n",
              "f26    0\n",
              "f27    0\n",
              "f28    0\n",
              "f29    0\n",
              "f30    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-uN0ZUsu3YA",
        "colab_type": "code",
        "outputId": "e4aa67eb-6a05-462c-933d-f991d87f7d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "## Print the number of features with missing entries\n",
        "\n",
        "# Function to calculate missing values by column \n",
        "def missing_values_table(df):\n",
        "        # Total missing values\n",
        "        mis_val = df.isnull().sum()\n",
        "        \n",
        "        # Percentage of missing values\n",
        "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "        \n",
        "        # Make a table with the results\n",
        "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "        \n",
        "        # Rename the columns\n",
        "        mis_val_table_ren_columns = mis_val_table.rename(\n",
        "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "        \n",
        "        # Sort the table by percentage of missing descending\n",
        "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "        '% of Total Values', ascending=False).round(1)\n",
        "        \n",
        "        # Print some summary information\n",
        "        print (\"Selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
        "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "              \" columns that have missing values.\")\n",
        "        \n",
        "        # Return the dataframe with missing information\n",
        "        return mis_val_table_ren_columns\n",
        "    \n",
        "missing_values = missing_values_table(X)\n",
        "print(missing_values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected dataframe has 30 columns.\n",
            "There are 1 columns that have missing values.\n",
            "     Missing Values  % of Total Values\n",
            "f21               3                2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyHFJQx1vMit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Taking care of Missing Data of training set\n",
        "#Fill the missing entries with mean value\n",
        "from sklearn.preprocessing import Imputer\n",
        "imputer=Imputer(missing_values='NaN',strategy='mean',axis=0)\n",
        "imputer=imputer.fit(X.iloc[:,])\n",
        "X.iloc[:,]=imputer.transform(X.iloc[:,])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v57CjQF8viEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Normalization\n",
        "#Feature Scaling of training and testing data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X=StandardScaler()\n",
        "X.iloc[:,]=sc_X.fit_transform(X.iloc[:,])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FfnwAATvoaN",
        "colab_type": "code",
        "outputId": "efea9518-fbe9-41ef-c327-63d8362cb946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "X.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>...</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.012045</td>\n",
              "      <td>-0.828471</td>\n",
              "      <td>-0.078207</td>\n",
              "      <td>-0.128934</td>\n",
              "      <td>-1.192851</td>\n",
              "      <td>-0.889448</td>\n",
              "      <td>-0.804495</td>\n",
              "      <td>-0.510419</td>\n",
              "      <td>-0.800921</td>\n",
              "      <td>-1.091524</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.268057</td>\n",
              "      <td>-0.979162</td>\n",
              "      <td>-0.302190</td>\n",
              "      <td>-0.329204</td>\n",
              "      <td>-1.286308</td>\n",
              "      <td>-1.021517</td>\n",
              "      <td>-1.012625</td>\n",
              "      <td>-0.437518</td>\n",
              "      <td>-1.230951</td>\n",
              "      <td>-1.088071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.624442</td>\n",
              "      <td>-1.114886</td>\n",
              "      <td>1.630477</td>\n",
              "      <td>1.641820</td>\n",
              "      <td>0.323186</td>\n",
              "      <td>0.485555</td>\n",
              "      <td>1.283885</td>\n",
              "      <td>1.329571</td>\n",
              "      <td>-0.016141</td>\n",
              "      <td>-0.657108</td>\n",
              "      <td>...</td>\n",
              "      <td>1.163573</td>\n",
              "      <td>-1.377066</td>\n",
              "      <td>1.179701</td>\n",
              "      <td>1.039790</td>\n",
              "      <td>0.179959</td>\n",
              "      <td>-0.372122</td>\n",
              "      <td>0.593109</td>\n",
              "      <td>0.642487</td>\n",
              "      <td>-0.879757</td>\n",
              "      <td>-0.495318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.306978</td>\n",
              "      <td>-0.819791</td>\n",
              "      <td>-0.285081</td>\n",
              "      <td>-0.359479</td>\n",
              "      <td>-1.301559</td>\n",
              "      <td>-0.175246</td>\n",
              "      <td>0.264042</td>\n",
              "      <td>-0.335835</td>\n",
              "      <td>-1.360968</td>\n",
              "      <td>-0.581049</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.457565</td>\n",
              "      <td>-0.921888</td>\n",
              "      <td>-0.419581</td>\n",
              "      <td>-0.473349</td>\n",
              "      <td>-1.464847</td>\n",
              "      <td>-0.204272</td>\n",
              "      <td>0.280910</td>\n",
              "      <td>-0.174843</td>\n",
              "      <td>-1.441975</td>\n",
              "      <td>-0.832722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.993416</td>\n",
              "      <td>-1.574886</td>\n",
              "      <td>-0.973285</td>\n",
              "      <td>-0.859547</td>\n",
              "      <td>0.298213</td>\n",
              "      <td>-0.539223</td>\n",
              "      <td>-0.823831</td>\n",
              "      <td>-0.672228</td>\n",
              "      <td>-0.048246</td>\n",
              "      <td>0.346289</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.930397</td>\n",
              "      <td>-1.530802</td>\n",
              "      <td>-0.922418</td>\n",
              "      <td>-0.784042</td>\n",
              "      <td>0.046270</td>\n",
              "      <td>-0.623582</td>\n",
              "      <td>-0.900548</td>\n",
              "      <td>-0.641851</td>\n",
              "      <td>0.002850</td>\n",
              "      <td>-0.468375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.236508</td>\n",
              "      <td>0.230398</td>\n",
              "      <td>-0.285832</td>\n",
              "      <td>-0.302804</td>\n",
              "      <td>-1.656329</td>\n",
              "      <td>-0.944098</td>\n",
              "      <td>-0.838333</td>\n",
              "      <td>-0.840662</td>\n",
              "      <td>-0.358591</td>\n",
              "      <td>-0.649795</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.344986</td>\n",
              "      <td>0.399939</td>\n",
              "      <td>-0.335198</td>\n",
              "      <td>-0.391930</td>\n",
              "      <td>-0.924053</td>\n",
              "      <td>-0.626096</td>\n",
              "      <td>-0.944189</td>\n",
              "      <td>-0.978058</td>\n",
              "      <td>-0.382232</td>\n",
              "      <td>-0.647793</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5        f6        f7  \\\n",
              "0 -0.012045 -0.828471 -0.078207 -0.128934 -1.192851 -0.889448 -0.804495   \n",
              "1  1.624442 -1.114886  1.630477  1.641820  0.323186  0.485555  1.283885   \n",
              "2 -0.306978 -0.819791 -0.285081 -0.359479 -1.301559 -0.175246  0.264042   \n",
              "3 -0.993416 -1.574886 -0.973285 -0.859547  0.298213 -0.539223 -0.823831   \n",
              "4 -0.236508  0.230398 -0.285832 -0.302804 -1.656329 -0.944098 -0.838333   \n",
              "\n",
              "         f8        f9       f10  ...       f21       f22       f23       f24  \\\n",
              "0 -0.510419 -0.800921 -1.091524  ... -0.268057 -0.979162 -0.302190 -0.329204   \n",
              "1  1.329571 -0.016141 -0.657108  ...  1.163573 -1.377066  1.179701  1.039790   \n",
              "2 -0.335835 -1.360968 -0.581049  ... -0.457565 -0.921888 -0.419581 -0.473349   \n",
              "3 -0.672228 -0.048246  0.346289  ... -0.930397 -1.530802 -0.922418 -0.784042   \n",
              "4 -0.840662 -0.358591 -0.649795  ... -0.344986  0.399939 -0.335198 -0.391930   \n",
              "\n",
              "        f25       f26       f27       f28       f29       f30  \n",
              "0 -1.286308 -1.021517 -1.012625 -0.437518 -1.230951 -1.088071  \n",
              "1  0.179959 -0.372122  0.593109  0.642487 -0.879757 -0.495318  \n",
              "2 -1.464847 -0.204272  0.280910 -0.174843 -1.441975 -0.832722  \n",
              "3  0.046270 -0.623582 -0.900548 -0.641851  0.002850 -0.468375  \n",
              "4 -0.924053 -0.626096 -0.944189 -0.978058 -0.382232 -0.647793  \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NpHk3lbwHg-",
        "colab_type": "text"
      },
      "source": [
        "###Explanation: \n",
        "\n",
        "The total number of features in the dataset are 30. We can see based on the merged dataset the the label is moderately imbalanced. There are 72 benign cases (0) and 48 malignant cases (1). \n",
        "\n",
        "Also, there is once column with missing values. The missing values have been replaced using the mean of overall column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_xgtidwb3H",
        "colab_type": "text"
      },
      "source": [
        "###1.2 Logistic Regression\n",
        "\n",
        "Train logistic regression models with L1 regularization and L2 regularization using alpha = 0.1\n",
        "and lambda = 0.1. Report accuracy, precision, recall, f1-score and print the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xkPESQgXz3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Sklearn Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## Performance Measures\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9702J0ODyvQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing the dataset\n",
        "#X_train,X_test\n",
        "X_train=pd.read_csv('/content/drive/My Drive/app/train_wbcd.csv')\n",
        "X_test=pd.read_csv('/content/drive/My Drive/app/test_wbcd.csv')\n",
        "#Removing patient_id\n",
        "X_train=X_train.drop(\"Patient_ID\",axis=1)\n",
        "X_test=X_test.drop(\"Patient_ID\",axis=1)\n",
        "\n",
        "#Taking care of Missing Data of training set\n",
        "#Fill the missing entries with mean value\n",
        "from sklearn.preprocessing import Imputer\n",
        "imputer=Imputer(missing_values='NaN',strategy='mean',axis=0)\n",
        "imputer=imputer.fit(X_train.iloc[:,1:])\n",
        "X_train.iloc[:,1:]=imputer.transform(X_train.iloc[:,1:])\n",
        "\n",
        "#Taking care of Missing Data of testing set\n",
        "imputer=Imputer(missing_values='NaN',strategy='mean',axis=0)\n",
        "imputer=imputer.fit(X_test.iloc[:,1:])\n",
        "X_test.iloc[:,1:]=imputer.transform(X_test.iloc[:,1:])\n",
        "\n",
        "#Normalization\n",
        "#Feature Scaling of training and testing data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X=StandardScaler()\n",
        "X_train.iloc[:,1:]=sc_X.fit_transform(X_train.iloc[:,1:])\n",
        "X_test.iloc[:,1:]=sc_X.transform(X_test.iloc[:,1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9WqEZ990Evt",
        "colab_type": "code",
        "outputId": "e8f8cc99-5cca-4154-e4a3-6ba8535700d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#y_train,y_test\n",
        "y_train=pd.DataFrame(X_train.iloc[:,0])\n",
        "y_test=pd.DataFrame(X_test.iloc[:,0])\n",
        "\n",
        "#Encoding Categorical Data (Diagnosis)\n",
        "#Malignant(M)-1\n",
        "#Benign(B)-0\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_y=LabelEncoder()\n",
        "y_train=labelencoder_y.fit_transform(y_train)\n",
        "y_test=labelencoder_y.fit_transform(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0zckEUb16xg",
        "colab_type": "code",
        "outputId": "4faf8784-01d7-44e3-c494-3d49deacfc41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#L1 Regularization\n",
        "alpha_val=0.1\n",
        "classifier1= LogisticRegression(C=1/alpha_val,penalty='l1',random_state=0)\n",
        "classifier1.fit(X_train.iloc[:,1:],y_train)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_pred_l1=classifier1.predict(X_test.iloc[:,1:])\n",
        "y_pred_l1=pd.DataFrame(y_pred_l1)\n",
        "\n",
        "#Performance of our training data\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test,y_pred_l1))\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test,y_pred_l1))\n",
        "\n",
        "#Precision\n",
        "from sklearn.metrics import precision_score\n",
        "print(\"Precision Score:\")\n",
        "print(precision_score(y_test,y_pred_l1))\n",
        "\n",
        "#Recall\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Recall Score:\")\n",
        "print(recall_score(y_test,y_pred_l1))\n",
        "\n",
        "#F-1 score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1 Score:\")\n",
        "print(f1_score(y_test,y_pred_l2))\n",
        "\n",
        "#Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test,y_pred_l1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[14  0]\n",
            " [ 1  5]]\n",
            "Accuracy Score:\n",
            "0.95\n",
            "Precision Score:\n",
            "1.0\n",
            "Recall Score:\n",
            "0.8333333333333334\n",
            "f1 Score:\n",
            "0.9090909090909091\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "   micro avg       0.95      0.95      0.95        20\n",
            "   macro avg       0.97      0.92      0.94        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAbFubBtze1s",
        "colab_type": "code",
        "outputId": "dc103a8e-60d1-4c88-bef4-8bb9713cbfb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#L2 Regularization\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lambda_val=0.1\n",
        "classifier2= LogisticRegression(C=1/lambda_val,penalty='l2',random_state=0)\n",
        "classifier2.fit(X_train.iloc[:,1:],y_train)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_pred_l2=classifier.predict(X_test.iloc[:,1:])\n",
        "y_pred_l2=pd.DataFrame(y_pred_l2)\n",
        "\n",
        "#Performance of our training data\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test,y_pred_l2))\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test,y_pred_l2))\n",
        "\n",
        "#Precision\n",
        "from sklearn.metrics import precision_score\n",
        "print(\"Precision Score:\")\n",
        "print(precision_score(y_test,y_pred_l2))\n",
        "\n",
        "#Recall\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Recall Score:\")\n",
        "print(recall_score(y_test,y_pred_l2))\n",
        "\n",
        "#F-1 score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1 Score:\")\n",
        "print(f1_score(y_test,y_pred_l2))\n",
        "\n",
        "#Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test,y_pred_l2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[14  0]\n",
            " [ 1  5]]\n",
            "Accuracy Score:\n",
            "0.95\n",
            "Precision Score:\n",
            "1.0\n",
            "Recall Score:\n",
            "0.8333333333333334\n",
            "f1 Score:\n",
            "0.9090909090909091\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "   micro avg       0.95      0.95      0.95        20\n",
            "   macro avg       0.97      0.92      0.94        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diAVnzaw3IMb",
        "colab_type": "code",
        "outputId": "fb4e7a9b-727f-4977-9330-069369afb1d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "## using alpha = .1\n",
        "\n",
        "alpha_val = 0.5\n",
        "\n",
        "## Train logistic regression using Elastic Net (Both L1 and L2 together)\n",
        "\n",
        "classifier3 = SGDClassifier(loss='log', penalty='elasticnet', alpha = alpha_val, l1_ratio=0.5, max_iter=1000,random_state=0)\n",
        "classifier3.fit(X_train.iloc[:,1:],y_train)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_pred_l3=classifier.predict(X_test.iloc[:,1:])\n",
        "y_pred_l3=pd.DataFrame(y_pred_l3)\n",
        "\n",
        "#Performance of our training data\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test,y_pred_l3))\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test,y_pred_l3))\n",
        "\n",
        "#Precision\n",
        "from sklearn.metrics import precision_score\n",
        "print(\"Precision Score:\")\n",
        "print(precision_score(y_test,y_pred_l3))\n",
        "\n",
        "#Recall\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Recall Score:\")\n",
        "print(recall_score(y_test,y_pred_l3))\n",
        "\n",
        "#F-1 score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1 Score:\")\n",
        "print(f1_score(y_test,y_pred_l3))\n",
        "\n",
        "#Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test,y_pred_l3))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[14  0]\n",
            " [ 1  5]]\n",
            "Accuracy Score:\n",
            "0.95\n",
            "Precision Score:\n",
            "1.0\n",
            "Recall Score:\n",
            "0.8333333333333334\n",
            "f1 Score:\n",
            "0.9090909090909091\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "   micro avg       0.95      0.95      0.95        20\n",
            "   macro avg       0.97      0.92      0.94        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYs07uDGcrMP",
        "colab_type": "text"
      },
      "source": [
        "**Explanation**\n",
        "\n",
        "We can see from the performance matrix above that Elastic Net performs best when it comes to accuracy, however it is not as precise as the L1 and L2 model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeCcKqh_eKZo",
        "colab_type": "text"
      },
      "source": [
        "###1.3 Choosing the best hyper-parameter:\n",
        "\n",
        "\n",
        "To choose the best hyperparameter (alpha/lambda) value, you have to do the following was done:\n",
        "\n",
        "• For each value of hyperparameter, performed 100 random splits of training data into training and validation\n",
        "data.\n",
        "\n",
        "• Found the average validation accuracy for each 100 train/validate pairs.\n",
        "\n",
        "The best hyperparameter was chosen with the maximum validation accuracy. Using the best alpha and lambda parameter, a new models were re-trained. \n",
        "\n",
        "Also evaluated the prediction performance on the test data and report the following:\n",
        "\n",
        "• Precision\n",
        "\n",
        "• Accuracy\n",
        "\n",
        "• The top 5 features selected in decreasing order of feature weights.\n",
        "\n",
        "• Confusion matrix\n",
        "\n",
        "Finally, it was explained whether we have build underfit or overfit models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vktmCUn-7gE7",
        "colab_type": "code",
        "outputId": "14961745-256c-4e11-a046-6eb7343d2e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#1.3 Hyperparameter Tuning\n",
        "#Best value of alpha\n",
        "#L1 Model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters=[{'C':[0.1,1,3,10,33,100,333,1000,3333,10000,33333]}]\n",
        "grid_search=GridSearchCV(estimator=classifier1,\n",
        "                         param_grid=parameters,\n",
        "                         scoring='accuracy',\n",
        "                         cv=10)\n",
        "grid_search.fit(X_train.iloc[:,1:],y_train)\n",
        "best_accuracy= grid_search.best_score_\n",
        "print(best_accuracy)\n",
        "best_parameters = grid_search.best_params_\n",
        "print(best_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "{'C': 33}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAZH1ica7ux7",
        "colab_type": "code",
        "outputId": "511f9864-5132-4c9d-af5f-3d3d495f7e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Best value of lambda\n",
        "#L2 Model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters=[{'C':[0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,33]}]\n",
        "grid_search=GridSearchCV(estimator=classifier2,\n",
        "                         param_grid=parameters,\n",
        "                         scoring='accuracy',\n",
        "                         cv=10)\n",
        "grid_search.fit(X_train.iloc[:,1:],y_train)\n",
        "best_accuracy= grid_search.best_score_\n",
        "print(best_accuracy)\n",
        "best_parameters = grid_search.best_params_\n",
        "print(best_parameters)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "{'C': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_by6BM0W-LZf",
        "colab_type": "code",
        "outputId": "96d2a69c-99de-480b-8411-5debf6e5221a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "#1.3 \n",
        "#Re-Train model with Alpha=1\n",
        "#Logistic Regression Models\n",
        "#L1 Regularization\n",
        "alpha_val=33\n",
        "classifier1= LogisticRegression(C=1/alpha_val,penalty='l1',random_state=0)\n",
        "classifier1.fit(X_train.iloc[:,1:],y_train)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_pred_l1=classifier1.predict(X_test.iloc[:,1:])\n",
        "y_pred_l1=pd.DataFrame(y_pred_l1)\n",
        "print(classifier1.coef_)\n",
        "print('\\n')\n",
        "print(classifier1.intercept_)\n",
        "\n",
        "#Performance of our training data\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test,y_pred_l1))\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test,y_pred_l1))\n",
        "\n",
        "#Precision\n",
        "from sklearn.metrics import precision_score\n",
        "print(\"Precision Score:\")\n",
        "print(precision_score(y_test,y_pred_l1))\n",
        "\n",
        "#Recall\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Recall Score:\")\n",
        "print(recall_score(y_test,y_pred_l1))\n",
        "\n",
        "#F-1 score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1 Score:\")\n",
        "print(f1_score(y_test,y_pred_l2))\n",
        "\n",
        "#Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test,y_pred_l1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.27471817 0.         0.        ]]\n",
            "\n",
            "\n",
            "[0.]\n",
            "Confusion Matrix:\n",
            "[[14  0]\n",
            " [ 1  5]]\n",
            "Accuracy Score:\n",
            "0.95\n",
            "Precision Score:\n",
            "1.0\n",
            "Recall Score:\n",
            "0.8333333333333334\n",
            "f1 Score:\n",
            "0.9090909090909091\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "   micro avg       0.95      0.95      0.95        20\n",
            "   macro avg       0.97      0.92      0.94        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnX-TkYK-srK",
        "colab_type": "code",
        "outputId": "18f018f5-7b87-4e14-99f4-684942bdbcc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "#Re-Train model with Lambda=1\n",
        "#Logistic Regression Models\n",
        "#L2 Regularization\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lambda_val=3\n",
        "classifier2= LogisticRegression(C=1/lambda_val,penalty='l2',random_state=0)\n",
        "classifier2.fit(X_train.iloc[:,1:],y_train)\n",
        "\n",
        "#Predicting the test set results\n",
        "y_pred_l2=classifier.predict(X_test.iloc[:,1:])\n",
        "y_pred_l2=pd.DataFrame(y_pred_l2)\n",
        "print(classifier2.coef_)\n",
        "print('\\n')\n",
        "print(classifier2.intercept_)\n",
        "\n",
        "#Performance of our training data\n",
        "#Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test,y_pred_l2))\n",
        "\n",
        "#Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy Score:\")\n",
        "print(accuracy_score(y_test,y_pred_l2))\n",
        "\n",
        "#Precision\n",
        "from sklearn.metrics import precision_score\n",
        "print(\"Precision Score:\")\n",
        "print(precision_score(y_test,y_pred_l2))\n",
        "\n",
        "#Recall\n",
        "from sklearn.metrics import recall_score\n",
        "print(\"Recall Score:\")\n",
        "print(recall_score(y_test,y_pred_l2))\n",
        "\n",
        "#F-1 score\n",
        "from sklearn.metrics import f1_score\n",
        "print(\"f1 Score:\")\n",
        "print(f1_score(y_test,y_pred_l2))\n",
        "\n",
        "#Classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test,y_pred_l2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.37743065  0.53907772  0.37668186  0.36002526  0.07570476  0.10009627\n",
            "   0.33332417  0.37879212  0.12661697 -0.21215823  0.24418852  0.04048929\n",
            "   0.22610686  0.28189845 -0.03186669 -0.15915038 -0.09710701 -0.01818867\n",
            "  -0.21511658 -0.10898513  0.42729335  0.69537222  0.43055565  0.39849811\n",
            "   0.40970225  0.26032052  0.39212078  0.41040213  0.39586109  0.32452514]]\n",
            "\n",
            "\n",
            "[-0.08404126]\n",
            "Confusion Matrix:\n",
            "[[14  0]\n",
            " [ 1  5]]\n",
            "Accuracy Score:\n",
            "0.95\n",
            "Precision Score:\n",
            "1.0\n",
            "Recall Score:\n",
            "0.8333333333333334\n",
            "f1 Score:\n",
            "0.9090909090909091\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "   micro avg       0.95      0.95      0.95        20\n",
            "   macro avg       0.97      0.92      0.94        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-DlFpsGgq5i",
        "colab_type": "text"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "Based on the above two model accuracies with best alpha/ lambda values, we can see that L1 regularized model with alpha 333 gives us the best accuracy of 95% which is higher than the average validation accuracy score of 85%. Also, there is no sign of underfitting/ overfitting since the accuracy was tested on an unseen test data and still maintains a moderately high accuracy.\n",
        "\n",
        "However, it should be noted that since it is apparently a cancer classification dataset, precision might be a more important performance indicator than accuracy, but alarmingly, we have a relatively low precision (83.33%). We need to evaluate building better models in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_2K9Cb5mwQL",
        "colab_type": "text"
      },
      "source": [
        "#End"
      ]
    }
  ]
}